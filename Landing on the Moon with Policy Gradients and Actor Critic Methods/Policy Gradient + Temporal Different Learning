Temporal Diffrence Learning
특정 시간 간격(사용자 지정)마다 배우기 진행

Temporal Difference + Policy Gradient > Actor Critic

Agent : actor, critic networks

Actor network : generates actions to get rewards
행위자 네트워크 : 각 행동에 따른 보상을 생성함
common input layers with distinct actor/critic output
입력 레이어 : 각각 actor, critic output

Simpler than two distinct networks
각각 다른 networks를 단순화

V(S_t), V(S_t+1)계산, V_terminal=0

delta : R_t + gamma *V(S_t+1, w)-V(S_t,w)
G_t in loss for actor
Critic loss : delta**2

weights를 profitable actions들이 되도록 조정
accuracy of estimate

이것을 모두 합친 알고리즘은 
actor critic network 초기화(theta, w)
episode 반복
environment 초기화
각각 episode step에서
action 선택, : 선택의 기준은 actor network
action을 선정하고 reward를 받고 새 state 적용
delta 계산
actor, critic loss 계산 > back propagate
