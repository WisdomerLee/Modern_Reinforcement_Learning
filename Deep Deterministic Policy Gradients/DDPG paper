Deep Q-Learning >
Actor-critic : model-free algorithm, policy gradient이용

policies를 찾는 것

Deep Learning + Reinforcement Learning : Deep Q Network
하지만 DQN은 관측하는 파라미터(high-dimensional observation space)는 몹시 많으나 행동으로 나타나는 것(low-dimensional action space)들은 매우 낮은 차원으로 나온다는 단점이 있음

DQN자체는 바로 action-value function의 최대 값을 찾는 것으로는 쓸 수 없음

DQN을 연속적으로 action space의 형태들을 나타내도록 접근시도
한계가 있음 : 차원의 저주 문제 : 자유도가 높아질 수록 행동의 갯수는 지수함수 수준으로 급증하는 문제가 있고 (예를 들어 사람의 팔과 같은 경우 7개의 자유도 시스템인데 각각의 관절의 결합으로 나타나는 행동들의 가짓수는 3(관절의 위치는 3차원 상으로 존재하므로)^7=2187개가 됨...)
이 현상은 보다 정밀한 상태로 행동들을 제어해야 하는 상황이 될 때 더 심화됨
매우 많은 행동들이 모인 공간(action space)의 차원이 높으면 이것들을 효과적으로 탐험하는 것이 쉽지 않게 됨 : DQN을 성공적으로 학습시키려면 상호작용이 가능한 상태여야 함
또한 action space의 구분된 정도들은 action domain의 구조 정보를 알려주게 되고 이것이 문제를 푸는데 효과적이될 수 있음

actor-critic approach와 Deep Q Network의 결합!
DQN은 value function이 매우 많고 선형적이지 않은 함수들의 접근이 많아 어렵고 안정적이지 않은 문제가 있음
이것을 2가지 새 혁신적인 방법으로 문제를 해결할 수 있는데...
1. network : trained off-policy with sample 네트워크의 학습은 off-policy 상태로 학습하고 replay buffer는 샘플간 상관관계를 줄여줌
2. network가 학습 대상인 Q network로 학습을 시작할 때 temporal difference backup된 상태로 학습을 시작함
batch normalization에서 사용했던 생각과 비슷함

해당 모델은 Deep DPG라고 명명함
같은 hyper-parameter와 같은 network structure를 가진 애들로 경쟁적인 policies들을 모든 상황에 대해 학습하고(low-dimensional observation)

actor-critic architecture, learning algorithm은 매우 적은 moving part로 하여 쉽게 적용하고 보다 복잡하고 큰 network에 적용하기 좋게 만듦

DQN : action space얻을 때 좋음
DDPG : replay buffer, target network 사용
buffer, network를 위한 클래스 필요

MDP : 이미 학습했다고 가정
Deterministic policy : sample sate with stochastic policy > off policy learning
deterministic, stochastic action을 같이 얻을 수 있는 방법을 찾아야 함
