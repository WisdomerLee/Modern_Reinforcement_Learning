기본

Agent, Environment, Action의 세 요소!를 통해 학습을 시킴

Agent의 행동을 학습하게 하는 것인데

Agent가 주어진 환경에서 어떤(최적의) 행동을 하도록 유도 하는 것이 강화 학습의 목적

상태는 오직 바로 직전의 상태와 행동에 의존함
Markov Decision Process

Episodic Returns
Agent의 상태에 따라 보상을 주어줌
그리고 보상이 높은 방향으로 agent가 행동하도록...
바로 직전의 상태가 더 보상이 높다면?
agent는 행동을 멈추게 될 것임

그래서 Reward Discounting을 도입하여

같은 상태로 계속 존재할 경우 reward를 조금씩 감소시켜 새 행동을 하도록 유도함!

Agent는 각 행동에 따른 상태(states)들이 존재하고 > 이는 확률에 따라 상태가 조정될 수 있음

Value와 Action Value Function이 있음

경험을 통해 배우는 것
환경과 상호작용을 하면 그에 따른 보상을 쭉 추적함

Bellman Equation을 통해 반복된 상호작용으로 인한 보상의 형태를 결정지음
Equation 속에 recursion 관련 부분이 있는데 

agent는 확률적으로 평균적으로 존재하게 되는 확률 상태를 반복되는 과정에서 그 weight를 조정하여 보다 더 나은 보상을 얻도록 행동함!

최적화 시키는 과정은
policies()를 비교
알려진 Dynamics > Model 기반
알 수 없는 Dynamics > Model 없음 : Free Learning

Explore Exploit Dilemma
Epsilon greedy(Q learning) , Approximate Policy Directly
하나의 policy > 행동을 만들고 value function을 업데이트함 > On policy : 같은 행동은 같은 결과를 가짐
하나의 policy > 행동을 만들고 또다른 policy updates value function을 업데이트 > Off policy : 같은 행동이 다른 결과 값을 가질 수 있음

Epsilon Greedy(Q Learning) : off policy learning
policy gradient : on policy learning

reward의 변화를 꾸준히 추적하여 value와 action value function을 평가
Recursive relationship between functions
함수들간의 관계를 반복하여 재조정

주어진 환경에서 여러 행동을 반복하여 학습
Policy gradient, actor critic : on policy model free

1.학습시킬 policy를 초기화
2.value function 각각 초기화
3.state space의 모든 가능한 state를 초기화
4. 매우 큰 숫자의 에피소드(reward를 얻도록 행동하게)를 반복
5.policy를 이용한 episode를 생성, 
