Temporal Difference Control: Q Learning

off policy, model free, bootstrapped
policy: agent의 행동양상들을 정의한 것인데 이것조차 정의하지 않음

epsilon greedy action selection for exploration
epsilon : agent가 얼마나 자주 행동을 선택하게 되는 빈도수

무작위 숫자를 생성하여 epsilon과 비교하여 epsilon이 크면 탐험(새로운 행동), 행동을 하고 그렇지 않으면 기존에 했던 행동을 지속
epsilon은 시간이 지나면서 점차 줄어들게 됨 (최소 값이 될 때까지)

greedy function을 업데이트!

Q Learning Update Rule

1. 처음 Q 값을 임의로 지정, terminal state는 0으로 지정
2. episode를 반복
환경을 설정
각 에피소드마다 
epsilon greedy policy에 따라 행동을 선택
행동을 취하고 reward를 얻고 환경의 새 상태를 얻게 됨
현재 상태를 새 상태로 설정
epsilon은 1에서 시작하여 0.01로 줄어들게 됨
알파, 감마 값 설정
