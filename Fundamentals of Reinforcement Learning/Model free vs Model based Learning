Markov property + policy > value function
Bellman Equation

state space에서 state의 확률 분포를 알아야 Bellman Equation을 적용할 수 있음
그렇다면 그 확률 분포가 알려지지 않은 경우엔 어떻게....?

policy pi : state에서 action을 취할 확률!!을 이용하여 
모든 reward들을 무수히 많은 반복을 통해 얻어낼 수 있음
역으로 terminal state에서 backward를 통해 역으로 확률 분포를 알아낸다면??
terminal state에서는 reward가 무조건 0이므로
여기서 recursion relation ship을 이용하여 다른 것들의 상태들을 역으로 추적해 감

이미 알려진 확률 분포라면
완성된 모델의 환경 complete model of environment
system의 equation을 풀기

이와 같이 algorithm의 바탕이 model이 되는 경우 dynamic programming이라고 함

알려지지 않은 확률 분포를 알아야 할 경우
Model free > Q Learning 기법을 씀
Q learning은 Model free 기법 중에 하나 일 뿐임을 기억

episode를 플레이 하여 확률 p를 계산함
: dynamic programming에 비해 매우 유연함

biological parallels
: 시도와 그에 따른 에러를 통해 배움을 수정
